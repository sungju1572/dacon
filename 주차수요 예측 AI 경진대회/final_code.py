# -*- coding: utf-8 -*-
"""light_gbm_poly_age_제외_최고성능_poly=2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZXxj4PX9qyJ_dK5wMUyeEYcz1KVAO9G-
"""

import pandas as pd
import glob
import os
import pandas as pd
from tqdm import tqdm
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler

train =pd.read_csv('train.csv')
test=pd.read_csv('test.csv')
sample_sub = pd.read_csv('sample_submission.csv')

sample_sub['num']= sample_sub['num'].astype('float')

#칼럼명 변셩
train.columns = [
    '단지코드', '총세대수', '임대건물구분', '지역', '공급유형', '전용면적', '전용면적별세대수', '공가수', '신분',
    '임대보증금', '임대료', '지하철', '버스',
    '단지내주차면수', '등록차량수'
]

test.columns = [
    '단지코드', '총세대수', '임대건물구분', '지역', '공급유형', '전용면적', '전용면적별세대수', '공가수', '신분',
    '임대보증금', '임대료', '지하철', '버스',
    '단지내주차면수'
]

#결측치 확인
train.isna().sum()
test.isna().sum()

train.info()
train.columns



"""
#임대보증금,임대료 제거
train = train.drop(["단지코드","임대보증금","임대료"], axis=1)

test = test.drop(["단지코드","임대보증금","임대료"], axis=1)

corr = train.corr()

train.isna().sum()
test.isna().sum()
"""

#
#임대보증금 float형으로 변경
train['임대보증금']=train['임대보증금'].replace("-",0)
test['임대보증금']=test['임대보증금'].replace("-",0)


train['임대보증금']=train['임대보증금'].astype('float')
test['임대보증금']=test['임대보증금'].astype('float')


#임대료 float형으로 변경
train['임대료']=train['임대료'].replace("-",0)
test['임대료']=test['임대료'].replace("-",0)

train['임대료']=train['임대료'].astype('float')
test['임대료']=test['임대료'].astype('float')


#신분 확인
rank = pd.concat([train.신분.value_counts(), test.신분.value_counts()], axis=1)


#결측치처리
train = train.fillna(0)
test.loc[(test.신분.isnull()) & (test.단지코드 == "C2411"), '신분'] = 'A'
test.loc[(test.신분.isnull()) & (test.단지코드 == "C2253"), '신분'] = 'C'
test["지하철"] = test["지하철"].fillna(0)
test = test.fillna(0)

test.isna().sum()

#단지코드는 동일하지만 다른 유형인것들 나누기
#train
code_name_train = train["단지코드"].unique()
for i in code_name_train:
    a = []
    b = train[train["단지코드"]==i]
    b_index = b.index
    b = b.reset_index(drop=True)
    for j in range(len(b)):
        a.append(b["신분"][j])
    if len(set(a)) >= 2 :
        for h in range(len(a)):
            b["단지코드"][h] = b["단지코드"][h]+"_"+a[h]
        train = pd.concat([train,b])
        train = train.reset_index(drop=True)
        train = train.drop(b_index)
        train = train.reset_index(drop=True)
            

#test
code_name_test = test["단지코드"].unique()
change_list = []
for i in code_name_test:
    a = []
    b = test[test["단지코드"]==i]
    b_index = b.index
    b = b.reset_index(drop=True)
    for j in range(len(b)):
        a.append(b["신분"][j])
    if len(set(a)) >= 2 :
        for h in range(len(a)):
            b["단지코드"][h] = b["단지코드"][h]+"_"+a[h]
            change_list.append(i)
        test = pd.concat([test,b])
        test = test.reset_index(drop=True)
        test = test.drop(b_index)
        test = test.reset_index(drop=True)
                        
    
####데이터 준비
"""
age.columns

age = age.drop(['10대미만(여자)', '10대미만(남자)', '10대(여자)', '10대(남자)'],axis=1)
"""

#축약
group_train = train.groupby(train.단지코드).mean()
group_test = test.groupby(test.단지코드).mean()


##칼럼추가
#1. 전용면적별 세대수 다 더해서 총임대가구수 컬럼 만들기
sum_train = train.groupby(train.단지코드).sum()
sum_test = test.groupby(test.단지코드).sum()

group_train["총임대가구수"] = sum_train["전용면적별세대수"]
group_test["총임대가구수"] = sum_test["전용면적별세대수"]

#2. 단지내 주차면수 / 총세대수 해서 가구당 주차면수 컬럼 만들기
group_train["가구당주차면수"] = group_train["단지내주차면수"] / group_train["총세대수"]
group_test["가구당주차면수"] = group_test["단지내주차면수"] / group_test["총세대수"]


#3. 총임대가구수 / 총 세대수 해서 임대비율 컬럼 만들기 
group_train["임대비율"] = group_train["총임대가구수"] / group_train["총세대수"]
group_test["임대비율"] = group_test["총임대가구수"] / group_test["총세대수"]



#min-max 스케일링
'''
group_train_col = group_train.columns
group_test_col = group_test.columns

group_train_idx = group_train.index
group_test_idx = group_test.index


group_train_label = group_train["등록차량수"]


x = group_train.values #returns a numpy array
min_max_scaler = MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
group_train = pd.DataFrame(x_scaled)

group_train.columns = group_train_col
group_train.index = group_train_idx
group_train["등록차량수"] = group_train_label 

x_t = group_test.values #returns a numpy array
min_max_scaler = MinMaxScaler()
x_scaled_t = min_max_scaler.fit_transform(x_t)
group_test = pd.DataFrame(x_scaled_t)

group_test.columns = group_test_col
group_test.index = group_test_idx
'''



##데이터 축약
#train

group_train["임대건물구분"] = ""
group_train["지역"] = ""
group_train["공급유형"] = ""
group_train["신분"] = ""

for i in range(len(train)):
    for j in range(len(group_train)):
        if train["단지코드"][i]== group_train.index[j]:
            group_train["임대건물구분"][j] = train["임대건물구분"][i]
            group_train["지역"][j] = train["지역"][i]
            group_train["공급유형"][j] = train["공급유형"][i]
            group_train["신분"][j] = train["신분"][i]


group_train_index = group_train.index

"""
group_train = pd.merge(group_train, age, how = 'inner', on="지역")
"""

#test



group_test["임대건물구분"] = ""
group_test["지역"] = ""
group_test["공급유형"] = ""
group_test["신분"] = ""

for i in range(len(test)):
    for j in range(len(group_test)):
        if test["단지코드"][i]== group_test.index[j]:
            group_test["임대건물구분"][j] = test["임대건물구분"][i]
            group_test["지역"][j] = test["지역"][i]
            group_test["공급유형"][j] = test["공급유형"][i]
            group_test["신분"][j] = test["신분"][i]

group_test_index = group_test.index

"""
group_test = pd.merge(group_test, age, how = 'inner', on="지역")
"""


###범주형 더미변수화
##임대건물구분
building_group_train_dummy = pd.get_dummies(group_train["임대건물구분"])
building_group_test_dummy = pd.get_dummies(group_test["임대건물구분"])

building_group_train_dummy.columns == building_group_test_dummy.columns 

group_train = group_train.join(building_group_train_dummy)
group_train = group_train.drop(["임대건물구분"], axis = 1)

group_test = group_test.join(building_group_test_dummy)
group_test = group_test.drop(["임대건물구분"], axis = 1)

##지역
area_group_train_dummy = pd.get_dummies(group_train["지역"])
area_group_test_dummy = pd.get_dummies(group_test["지역"])

#컬럼비교
area_group_train_dummy.columns  
area_group_test_dummy.columns 

#group_test에 없는 컬럼 채우기
area_group_test_dummy["서울특별시"] = 0
area_group_test_dummy = area_group_test_dummy.astype("uint8")

#group_train과 같게 재배열
area_group_test_dummy = area_group_test_dummy[area_group_train_dummy.columns]

#비교
area_group_train_dummy.columns == area_group_test_dummy.columns

#원래행에 더미변수 조인 / 기존컬럼 삭제
group_train = group_train.join(area_group_train_dummy)
group_train = group_train.drop(["지역"], axis=1)

group_test = group_test.join(area_group_test_dummy)
group_test = group_test.drop(["지역"], axis=1)


##공급유형
supply_group_train_dummy = pd.get_dummies(group_train["공급유형"])
supply_group_test_dummy = pd.get_dummies(group_test["공급유형"])

supply_group_train_dummy.columns
supply_group_test_dummy.columns 

supply_group_test_dummy["공공분양"] = 0
supply_group_test_dummy["공공임대(5년)"] = 0
supply_group_test_dummy["장기전세"] = 0

supply_group_test_dummy = supply_group_test_dummy.astype("uint8")

supply_group_test_dummy.info()

supply_group_test_dummy = supply_group_test_dummy[supply_group_train_dummy.columns] 

supply_group_train_dummy.columns == supply_group_test_dummy.columns  

group_train = group_train.join(supply_group_train_dummy)
group_train = group_train.drop(["공급유형"], axis = 1)

group_test = group_test.join(supply_group_test_dummy)
group_test = group_test.drop(["공급유형"], axis = 1)

##신분
rank_group_train_dummy = pd.get_dummies(group_train["신분"])
rank_group_test_dummy = pd.get_dummies(group_test["신분"])

rank_group_train_dummy.columns
rank_group_test_dummy.columns 

rank_group_test_dummy["B"] = 0
rank_group_test_dummy["F"] = 0
rank_group_test_dummy["O"] = 0


rank_group_test_dummy = rank_group_test_dummy[rank_group_train_dummy.columns] 

rank_group_test_dummy = rank_group_test_dummy.astype("uint8")

rank_group_test_dummy.columns == rank_group_train_dummy.columns  

group_train = group_train.join(rank_group_train_dummy)
group_train = group_train.drop(["신분"], axis=1)

group_test = group_test.join(rank_group_test_dummy)
group_test = group_test.drop(["신분"], axis=1)



#오류 난 행삭제
group_train = group_train.drop(index=['C1095', 'C2051', 'C1218', 'C1894', 'C2483', 'C1502', 'C1988'])


##라벨컬럼 이름변경
group_train = group_train.rename(columns={'등록차량수':'label'})
group_train

#폴리노미널

group_train_col = group_train.columns
group_test_col = group_test.columns

group_train_idx = group_train.index
group_test_idx = group_test.index

"""
group_train = group_train.drop(["지하철","임대비율"],axis=1)
group_test = group_test.drop(["지하철","임대비율"],axis=1)
"""
group_label = group_train["label"]
group_train = group_train.drop(["label"],axis=1)


group_train_dummy = group_train.iloc[:,12:]
group_test_dummy = group_test.iloc[:,12:]

group_train = group_train.iloc[:,0:12]
group_test = group_test.iloc[:,0:12]



from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)
train_poly = poly_features.fit_transform(group_train)
test_poly = poly_features.fit_transform(group_test)


train_poly = pd.DataFrame(train_poly)
test_poly = pd.DataFrame(test_poly)

train_poly.index = group_train_idx 
test_poly.index = group_test_idx 

train_poly["label"] = group_label

group_train = pd.concat([train_poly, group_train_dummy], axis = 1)
group_test = pd.concat([test_poly, group_test_dummy], axis = 1)

x_train = group_train.drop(["label"], axis = 1 ) #학습데이터
y_train = group_train["label"] #정답라벨
x_test = group_test #test데이터

### modeling package ###
!pip install optuna
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
import optuna
from sklearn import linear_model
from sklearn import metrics
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn import preprocessing
from sklearn.model_selection import cross_val_score
from optuna.samplers import RandomSampler, GridSampler, TPESampler
import sklearn
from scipy.misc import derivative
from sklearn.metrics import mean_squared_error
import pickle

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
import random
import lightgbm as lgb
import random

dtrain = lgb.Dataset(x_train,y_train)

class cparams():
    def __init__(self): 
        self.seed = 700
        self.num_iterations = 100 # Default = 100
        self.learning_rate = 0.1 # Default = 0.1
        self.num_leaves = 31 #Default = 31
        self.min_child_samples = 20 #Default = 20
        self.min_child_weight = 0.001 #Default = 0.001
        self.bagging_fraction = 1.0
        self.feature_fraction = 1.0
        self.bagging_freq = 0
        self.alpha = 0.25
        self.gamma = 2.0
        self.l1 = 0.0
        self.l2 = 0.0
        
    def calibrate(self,num_round):
        
        param = {'boosting_type': 'gbdt', 
                'objective': 'regression',
                'metric': 'rmse', 
                'learning_rate': self.learning_rate, 
                'num_leaves': self.num_leaves,     
                'min_data_in_leaf': self.min_child_samples,   
                'min_sum_hessian_in_leaf':self.min_child_weight, 
                'bagging_fraction': self.bagging_fraction, 
                'bagging_freq': self.bagging_freq,
                'feature_fraction': self.feature_fraction, 
                'lambda_l1': self.l1,
                'lambda_l2': self.l2,
                'seed': self.seed
        }
        
        # cv's seed used to generate folds passed to numpy.random.seed
        bst = lgb.cv(param, dtrain,num_boost_round=num_round, stratified=False, 
                     shuffle=True,early_stopping_rounds=100,verbose_eval=10,seed=700)
        return bst
    
    def get_param(self):
        
        param = {'boosting_type': 'gbdt', 
                 'objective': 'regression',
                'metric': 'mae', 
                'learning_rate': self.learning_rate, 
                'num_leaves': self.num_leaves,     
                'min_data_in_leaf': self.min_child_samples,   
                'min_sum_hessian_in_leaf':self.min_child_weight, 
                'bagging_fraction': self.bagging_fraction, 
                'bagging_freq': self.bagging_freq,
                'feature_fraction': self.feature_fraction, 
                'lambda_l1': self.l1,
                'lambda_l2': self.l2,
                'seed': self.seed
        }        
        
        return param

current_model = cparams()

params = {
    'objective': 'regression',
    'metric': 'mae', 
    "verbosity": 1,
    "boosting_type": "gbdt",
    'seed':700
}

eval_history = lgb.cv(
    params, dtrain, verbose_eval=20,
    stratified=False, num_boost_round=1000, early_stopping_rounds=100,
    nfold=5,seed=700)

print('Best score: ', eval_history['l1-mean'][-1])
print('Number of estimators: ', len(eval_history['l1-mean']))
current_model.num_iterations = len(eval_history['l1-mean'])

study_name2 = 'lgb_leaves'
study_leaves = optuna.create_study(study_name=study_name2,direction='minimize',sampler=TPESampler(0))

def opt_leaves(trial):
    
    params = {
        'objective': 'regression',
        'metric': 'mae', 
        "verbosity": 1,
        "boosting_type": "gbdt",
        'seed':700,
        'num_leaves':int(trial.suggest_loguniform("num_leaves", 3,32))
    }
    
    score = lgb.cv(
        params, dtrain, verbose_eval=0, 
        stratified=False, num_boost_round=current_model.num_iterations,
        nfold=5,seed=700)
    return -score['l1-mean'][-1]

study_leaves.optimize(opt_leaves, n_trials=50)

print('Total number of trials: ',len(study_leaves.trials))
trial_leaves = study_leaves.best_trial
print('Best score : {}'.format(-trial_leaves.value))
for key, value in trial_leaves.params.items():
    print("    {}: {}".format(key, value))

current_model.num_leaves = int(trial_leaves.params['num_leaves'])

study_name3 = 'lgb_child_weight_sample'
study_sample_weight = optuna.create_study(study_name=study_name3,direction='maximize',sampler=TPESampler(0))


def opt_sample_weight(trial):
    
    params = {
        'objective': 'regression',
        'metric': 'mae', 
        "verbosity": 1,
        "boosting_type": "gbdt",
        'seed':700,
        'num_leaves':current_model.num_leaves,
        'min_data_in_leaf':int(trial.suggest_discrete_uniform('data_in_leaf',4,32,q=2)),
        'min_sum_hessian_in_leaf':trial.suggest_discrete_uniform('min_hessian',0.001,0.003,q=0.0005)
    }
    
    score = lgb.cv(
        params, dtrain, verbose_eval=0, 
        stratified=False, num_boost_round=current_model.num_iterations,
        nfold=5,seed=700)
    return -score['l1-mean'][-1]

study_sample_weight.optimize(opt_sample_weight, n_trials=50)

print('Total number of trials: ',len(study_sample_weight.trials))
trial_sample_weight = study_sample_weight.best_trial
print('Best score : {}'.format(-trial_sample_weight.value))
for key, value in trial_sample_weight.params.items():
    print("    {}: {}".format(key, value))

current_model.min_child_samples = int(trial_sample_weight.params['data_in_leaf'])
current_model.min_child_weight = trial_sample_weight.params['min_hessian']

study_name4 = 'lgb_bagging'
study_bagging = optuna.create_study(study_name=study_name4,direction='maximize',sampler=TPESampler(0))


def opt_bagging(trial):
    
    params = {
        'objective': 'regression',
        'metric': 'mae',  
        "verbosity": 1,
        "boosting_type": "gbdt",
        'seed':700,
        'num_leaves':current_model.num_leaves,
        'min_data_in_leaf':current_model.min_child_samples,
        'min_sum_hessian_in_leaf':current_model.min_child_weight,
        'bagging_fraction': trial.suggest_discrete_uniform('bfrac',0.4,1.0,q=0.05),
        'bagging_freq': int(trial.suggest_discrete_uniform('bfreq',1,7,q=1.0)),
        'feature_fraction':trial.suggest_discrete_uniform('feature',0.4,1.0,q=0.05)
    }
    
    score = lgb.cv(
        params, dtrain, verbose_eval=0, 
        stratified=False, num_boost_round=current_model.num_iterations,
        nfold=5,seed=700)
    return -score['l1-mean'][-1]

study_bagging.optimize(opt_bagging, n_trials=50)

print('Total number of trials: ',len(study_bagging.trials))
trial_bagging = study_bagging.best_trial
print('Best score : {}'.format(trial_bagging.value))
for key, value in trial_bagging.params.items():
    print("    {}: {}".format(key, value))

study_name5 = 'l1_l2'
study_reg = optuna.create_study(study_name=study_name5,direction='maximize',sampler=TPESampler(0))

def opt_reg(trial):
    
    params = {
        'objective': 'regression',
        'metric': 'mae', 
        "verbosity": 1,
        "boosting_type": "gbdt",
        'seed':700,
        'num_leaves':current_model.num_leaves,
        'min_data_in_leaf':current_model.min_child_samples,
        'min_sum_hessian_in_leaf':current_model.min_child_weight,
        'bagging_fraction': current_model.bagging_fraction,
        'bagging_freq': current_model.bagging_freq,
        'feature_fraction':current_model.feature_fraction,
        'lambda_l1': trial.suggest_loguniform("lambda_l1", 1e-7, 10),
        'lambda_l2': trial.suggest_loguniform("lambda_l2", 1e-7, 10)
    }
    
    score = lgb.cv(
        params, dtrain, verbose_eval=0, 
        stratified=False, num_boost_round=current_model.num_iterations,
        nfold=5,seed=700)
    return -score['l1-mean'][-1]

study_reg.optimize(opt_reg, n_trials=50)

print('Total number of trials: ',len(study_reg.trials))
trial_reg = study_reg.best_trial
print('Best score : {}'.format(trial_reg.value))
for key, value in trial_reg.params.items():
    print("    {}: {}".format(key, value))

current_model.learning_rate = 0.05
lr1 = current_model.calibrate(10000) 
print('Best score: ', lr1['rmse-mean'][-1])
print('Number of estimators: ', len(lr1['rmse-mean']))

current_model.learning_rate = 0.01
lr2 = current_model.calibrate(10000) 
print('Best score: ', lr2['rmse-mean'][-1])
print('Number of estimators: ', len(lr2['rmse-mean']))

current_model.learning_rate = 0.005
lr3 = current_model.calibrate(10000) 
print('Best score: ', lr3['rmse-mean'][-1])
print('Number of estimators: ', len(lr3['rmse-mean']))

## Get Current Parameters
current_model.learning_rate = 0.005 #Based on what we found above
current_param = current_model.get_param()
for key, value in current_param.items():
    print("    {}: {}".format(key, value))

model = lgb.train(current_param, dtrain,num_boost_round=5000,
                          verbose_eval=20)

pred=model.predict(x_test)

#병합
pred = pd.DataFrame(pred)
pred["code"] = group_test_index

change_list_set = set(change_list)

#test 중 값이 나뉜것 평균값으로 합치기
for j in change_list_set:
    a = []
    for i in range(len(pred)):
        if pred["code"][i][0:5]== j:
            a.append(pred[0][i])
    if len(a)>=2:
        mean_a = np.mean(a)
        pred = pred.append({0 : mean_a, "code" : j}, ignore_index=True)
        
        
 
#test중 값 나뉘어있던것 평균으로 합치기
for i in pred["code"]:
    if len(i)>5:
        idx = pred[pred["code"]==i].index
        pred = pred.drop(idx)
        
                
pred = pred.reset_index(drop=True)


for i in range(len(sample_sub)):
   for j in range(len(sample_sub)):
       if sample_sub["code"][i] == pred["code"][j]:
              sample_sub["num"][i] = pred[0][j]





sample_sub.to_csv('sub.csv',index=False)



lgb_result = sample_sub['num']-1

lgb_result


#잔차검증

from sklearn.model_selection import KFold



cv = KFold(n_splits = 3, random_state=1,shuffle=True)




for t,v in cv.split(group_train):
    train_cv=group_train.iloc[t]       # 훈련용
    val_cv=group_train.iloc[v]         # 검증용 분리.
    
    train_x=train_cv.drop(['label'],axis=1)   # 훈련용 독립변수들의 데이터,
    train_y=train_cv['label']   # 훈련용 종속변수만 있는 데이터
     
    val_x=val_cv.drop(['label'],axis=1)      # 검증용 독립변수들의 데이터,
    val_y=val_cv['label']
    
    train_ds = lgb.Dataset(train_x,label=train_y)
    valid_ds = lgb.Dataset(val_x,label=val_y)

    model = lgb.train(current_param, train_ds,5000,valid_ds,verbose_eval=20)
    oof_preds= model.predict(val_x)
    
    
#잔차확인
error_hist=val_y - oof_preds


error_hist_df = pd.DataFrame(error_hist)

#잔차평균
error_hist_df.mean()


#잔차 상하위 20개 행 살펴보기
a = abs(error_hist_df).sort_values("label", ascending=False)
b = abs(error_hist_df).sort_values("label")


error_top_20 = a.head(20).index
error_best_20 = b.head(20).index


exam = pd.DataFrame()

for i in range(len(train)):
    for j in range(len(error_top_20)):
        if train["단지코드"][i] == error_top_20[j]:
            exam = pd.concat([exam,train.loc[i]],axis=1)

exam = exam.T



exam_2 = pd.DataFrame()

for i in range(len(train)):
    for j in range(len(error_top_20)):
        if train["단지코드"][i] == error_best_20[j]:
            exam_2 = pd.concat([exam_2,train.loc[i]],axis=1)

exam_2 = exam_2.T


#시각화
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from matplotlib import font_manager, rc


sns.distplot(x=error_hist)

plt.axvline(25)

######  잔차검증 ####